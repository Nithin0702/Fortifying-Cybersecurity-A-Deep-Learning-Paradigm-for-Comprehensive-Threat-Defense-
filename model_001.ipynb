{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nfstream\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pefile\n",
    "import seaborn as sns\n",
    "import struct\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.nn import TransformerEncoder, TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network data feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:26<00:00,  4.10it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]/home/mani/main_env/lib/python3.10/site-packages/nfstream/streamer.py:547: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(temp_file_path)\n",
      " 25%|██▌       | 5/20 [00:12<00:40,  2.69s/it]/home/mani/main_env/lib/python3.10/site-packages/nfstream/streamer.py:547: DtypeWarning: Columns (33,34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(temp_file_path)\n",
      " 75%|███████▌  | 15/20 [00:25<00:05,  1.18s/it]/home/mani/main_env/lib/python3.10/site-packages/nfstream/streamer.py:547: DtypeWarning: Columns (33,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(temp_file_path)\n",
      " 85%|████████▌ | 17/20 [00:30<00:05,  1.72s/it]/home/mani/main_env/lib/python3.10/site-packages/nfstream/streamer.py:547: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(temp_file_path)\n",
      "100%|██████████| 20/20 [00:36<00:00,  1.83s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/home/mani/Desktop/main_project/data/network_data/ids-dataset/'\n",
    "benign_dir = os.path.join(dataset_dir, 'benign/')\n",
    "malicious_dir = os.path.join(dataset_dir, 'malicious/')\n",
    "benign_files = os.listdir(benign_dir)\n",
    "malicious_files = os.listdir(malicious_dir)\n",
    "\n",
    "features = ['src_port', 'dst_port', 'protocol', 'ip_version', 'vlan_id',\n",
    "\t   'tunnel_id', 'bidirectional_first_seen_ms',\n",
    "\t   'bidirectional_last_seen_ms', 'bidirectional_duration_ms',\n",
    "\t   'bidirectional_packets', 'bidirectional_bytes', 'src2dst_first_seen_ms',\n",
    "\t   'src2dst_last_seen_ms', 'src2dst_duration_ms', 'src2dst_packets',\n",
    "\t   'src2dst_bytes', 'dst2src_first_seen_ms', 'dst2src_last_seen_ms',\n",
    "\t   'dst2src_duration_ms', 'dst2src_packets', 'dst2src_bytes',\n",
    "\t   'application_name', 'application_category_name', 'user_agent',\n",
    "\t   'content_type']\n",
    "\n",
    "def get_features_from_pacp_file(pcap_file):\n",
    "\tflow = nfstream.NFStreamer(source=pcap_file, statistical_analysis=False).to_pandas()\n",
    "\t# encode the application_name, application_category_name, user_agent, content_type\n",
    "\tflow['application_name'] = le.fit_transform(flow['application_name'])\n",
    "\tflow['application_category_name'] = le.fit_transform(flow['application_category_name'])\n",
    "\tflow['user_agent'] = le.fit_transform(flow['user_agent'])\n",
    "\tflow['content_type'] = le.fit_transform(flow['content_type'])\n",
    "\t# drop the columns which are not required\n",
    "\tflow = flow.drop(['id', 'expiration_id', 'src_ip', 'src_mac', 'src_oui', 'dst_ip', 'dst_mac', 'dst_oui', 'application_is_guessed', 'application_confidence', 'requested_server_name', 'client_fingerprint', 'server_fingerprint'], axis=1)\n",
    "\tflow = flow.drop([\"vlan_id\", \"tunnel_id\"], axis=1)\n",
    "\tflow = flow.fillna(0)\n",
    "\tflow = flow.drop_duplicates()\n",
    "\t# print the column name if it has string datatype\n",
    "\tfor col in flow.columns:\n",
    "\t\tif flow[col].dtype == 'object':\n",
    "\t\t\tprint(col)\n",
    "\tfeatures = flow.values.tolist()\n",
    "\treturn features\n",
    "\n",
    "# Create a DataFrame for benign features\n",
    "benign_features = []\n",
    "for file in tqdm(benign_files):\n",
    "\tfeatures = get_features_from_pacp_file(benign_dir + file)\n",
    "\tbenign_features.extend(features)\n",
    "\n",
    "# Create a DataFrame for malicious features\n",
    "malicious_features = []\n",
    "for file in tqdm(malicious_files):\n",
    "\tfeatures = get_features_from_pacp_file(malicious_dir + file)\n",
    "\tmalicious_features.extend(features)\n",
    "\n",
    "benign_labels = [0] * len(benign_features)\n",
    "malicious_labels = [1] * len(malicious_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in network_data.columns:\n",
    "\tprint(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### portable excutable file feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1082/1082 [01:02<00:00, 17.25it/s]\n",
      "100%|██████████| 10841/10841 [07:27<00:00, 24.25it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_features_from_pe(file_path):\n",
    "    # Extract header information\n",
    "    PEfile = pefile.PE(file_path)\n",
    "    feature_vector = [PEfile.OPTIONAL_HEADER.DATA_DIRECTORY[6].Size, PEfile.OPTIONAL_HEADER.DATA_DIRECTORY[6].VirtualAddress, PEfile.OPTIONAL_HEADER.MajorImageVersion, PEfile.OPTIONAL_HEADER.MajorOperatingSystemVersion, PEfile.OPTIONAL_HEADER.DATA_DIRECTORY[0].VirtualAddress,\n",
    "                      PEfile.OPTIONAL_HEADER.DATA_DIRECTORY[0].Size, PEfile.OPTIONAL_HEADER.DATA_DIRECTORY[12].VirtualAddress, PEfile.OPTIONAL_HEADER.DATA_DIRECTORY[2].Size, PEfile.OPTIONAL_HEADER.MajorLinkerVersion, PEfile.FILE_HEADER.NumberOfSections, PEfile.OPTIONAL_HEADER.SizeOfStackReserve, PEfile.OPTIONAL_HEADER.DllCharacteristics]\n",
    "\n",
    "    machine_type = PEfile.FILE_HEADER.Machine\n",
    "    num_sections = len(PEfile.sections)\n",
    "    entry_point = PEfile.OPTIONAL_HEADER.AddressOfEntryPoint\n",
    "    feature_vector.extend([machine_type, num_sections,  entry_point])\n",
    "\n",
    "    # Extract section information\n",
    "    section_info = [section.SizeOfRawData for section in PEfile.sections]\n",
    "    feature_vector += section_info\n",
    "\n",
    "    # Extract ASCII and Unicode strings\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    ascii_strings = re.findall(b'[ -~]{4,}', data)\n",
    "    unicode_strings = re.findall(b'[\\x20-\\x7E\\x80-\\xFE]{4,}', data)\n",
    "    feature_vector.extend([len(ascii_strings), len(unicode_strings)])\n",
    "\n",
    "    # Extract import information\n",
    "    imports = {}\n",
    "    for entry in PEfile.DIRECTORY_ENTRY_IMPORT:\n",
    "        try:\n",
    "            dll_name = entry.dll.decode('utf-8')\n",
    "            imports[dll_name] = [func.name.decode(\n",
    "                'utf-8') for func in entry.imports]\n",
    "        except:\n",
    "            dll_name = entry.dll\n",
    "    feature_vector.extend(list(imports.keys()))\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "benign_dir = \"/home/mani/Desktop/main_project/data/DikeDataset/files/benign\"\n",
    "malicious_dir = \"/home/mani/Desktop/main_project/data/DikeDataset/files/malware\"\n",
    "\n",
    "\n",
    "pe_data = pd.DataFrame(columns=['Name', 'MachineType', 'NumberOfSections', 'AddressOfEntryPoint',\n",
    "                  'SizeOfRawData', 'NumberOfAsciiStrings', 'NumberOfUnicodeStrings', 'Imports', 'Label'])\n",
    "\n",
    "for file_name in tqdm(os.listdir(benign_dir)):\n",
    "    file_path = os.path.join(benign_dir, file_name)\n",
    "    try:\n",
    "        features = get_features_from_pe(file_path)\n",
    "        pe_data.loc[len(pe_data.index)] = [file_name, features[0], features[1],\n",
    "                                 features[2], features[3], features[4], features[5], features[6], 0]\n",
    "    except Exception as e:\n",
    "        # print(f\"file {file_name} caused error\")\n",
    "        # print(e)\n",
    "        continue\n",
    "\n",
    "for file_name in tqdm(os.listdir(malicious_dir)):\n",
    "    file_path = os.path.join(malicious_dir, file_name)\n",
    "    try:\n",
    "        features = get_features_from_pe(file_path)\n",
    "        pe_data.loc[len(pe_data.index)] = [file_name, features[0], features[1],\n",
    "                                 features[2], features[3], features[4], features[5], features[6], 1]\n",
    "    except Exception as e:\n",
    "        # print(f\"file {file_name} caused error\")\n",
    "        # print(e)\n",
    "        continue\n",
    "\n",
    "# label encoding\n",
    "pe_data['Imports'] = le.fit_transform(pe_data['Imports'])\n",
    "\n",
    "# save data\n",
    "pe_data.to_csv(\"../data/csv/pe_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128293, 23)\n",
      "(128293,)\n",
      "(9876, 7)\n",
      "(9876,)\n",
      "   123  123.1  17  4  1533042976474  1533042976481  7  2  180  \\\n",
      "0  123    123  17  4  1533043015474  1533043015478  4  2  180   \n",
      "1  123    123  17  4  1533043146474  1533043146479  5  2  180   \n",
      "2  123    123  17  4  1533043143474  1533043143476  2  2  180   \n",
      "3  123    123  17  4  1533043237474  1533043237481  7  2  180   \n",
      "4  123    123  17  4  1533043277474  1533043277478  4  2  180   \n",
      "\n",
      "   1533042976474.1  ...  90  1533042976481.1  1533042976481.2  0.1  1.1  90.1  \\\n",
      "0    1533043015474  ...  90    1533043015478    1533043015478    0    1    90   \n",
      "1    1533043146474  ...  90    1533043146479    1533043146479    0    1    90   \n",
      "2    1533043143474  ...  90    1533043143476    1533043143476    0    1    90   \n",
      "3    1533043237474  ...  90    1533043237481    1533043237481    0    1    90   \n",
      "4    1533043277474  ...  90    1533043277478    1533043277478    0    1    90   \n",
      "\n",
      "   0.2  1.2  0.3  0.4  \n",
      "0    0    1    0    0  \n",
      "1    0    1    0    0  \n",
      "2    0    1    0    0  \n",
      "3    0    1    0    0  \n",
      "4    0    1    0    0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "network_data = pd.read_csv(\"../data/csv/network_data.csv\")\n",
    "pe_data = pd.read_csv(\"../data/csv/pe_data.csv\")\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_network = network_data.drop(['label'], axis=1)\n",
    "y_network = network_data['label']\n",
    "x_pe = pe_data.drop(['Name', 'Label'], axis=1)\n",
    "y_pe = pe_data['Label']\n",
    "\n",
    "# print shape\n",
    "print(x_network.shape)\n",
    "print(y_network.shape)\n",
    "print(x_pe.shape)\n",
    "print(y_pe.shape)\n",
    "\n",
    "print(x_network.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128293, 7)\n",
      "(9876, 7)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pca = PCA(n_components=7)\n",
    "\n",
    "transformed_x_network = pca.fit_transform(x_network)\n",
    "\n",
    "transformed_x_pe = pca.fit_transform(x_pe)\n",
    "\n",
    "# Check the shapes\n",
    "print(transformed_x_network.shape)\n",
    "print(transformed_x_pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no.of train samples: 100264\n",
      "no.of test samples: 25067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(benign_features + malicious_features, [0]*len(\n",
    "    benign_features) + [1]*len(malicious_features), test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = pd.DataFrame(X_train), pd.DataFrame(X_test), pd.DataFrame(y_train), pd.DataFrame(y_test)\n",
    "X_train.to_csv(\"../data/csv/network/trainset.csv\", index=False)\n",
    "X_test.to_csv(\"../data/csv/network/testset.csv\", index=False)\n",
    "y_train.to_csv(\"../data/csv/network/trainlabel.csv\", index=False)\n",
    "y_test.to_csv(\"../data/csv/network/testlabel.csv\", index=False)\n",
    "\n",
    "\n",
    "print(f\"no.of train samples: {len(X_train)}\")\n",
    "print(f\"no.of test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/csv/network/trainset.csv\")\n",
    "X_test = pd.read_csv(\"../data/csv/network/testset.csv\")\n",
    "y_train = pd.read_csv(\"../data/csv/network/trainlabel.csv\")\n",
    "y_test = pd.read_csv(\"../data/csv/network/testlabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Assuming your data has the shape (9876, 7)\n",
    "# X_train is your input data, and y_train is your binary labels (0 or 1)\n",
    "\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first LSTM layer with 100 units and input shape (timesteps, features)\n",
    "model.add(LSTM(100, input_shape=(\n",
    "    X_train.shape[1],1), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Adding dropout for regularization\n",
    "\n",
    "# Add a second LSTM layer with 50 units\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a third LSTM layer with 25 units\n",
    "model.add(LSTM(25))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a Dense layer with 50 units\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add the output layer with one unit and a sigmoid activation function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an optimizer of your choice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.any(np.isnan(X_train)))\n",
    "print(np.all(np.isfinite(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2507/2507 [==============================] - 81s 30ms/step - loss: 0.0489 - accuracy: 0.9861 - val_loss: 0.0159 - val_accuracy: 0.9969\n",
      "Epoch 2/10\n",
      "2507/2507 [==============================] - 73s 29ms/step - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.0155 - val_accuracy: 0.9963\n",
      "Epoch 3/10\n",
      "2507/2507 [==============================] - 74s 30ms/step - loss: 0.0121 - accuracy: 0.9964 - val_loss: 0.0085 - val_accuracy: 0.9981\n",
      "Epoch 4/10\n",
      "2507/2507 [==============================] - 74s 30ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0076 - val_accuracy: 0.9982\n",
      "Epoch 5/10\n",
      "2507/2507 [==============================] - 74s 30ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.0051 - val_accuracy: 0.9987\n",
      "Epoch 6/10\n",
      "2507/2507 [==============================] - 73s 29ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0066 - val_accuracy: 0.9984\n",
      "Epoch 7/10\n",
      "2507/2507 [==============================] - 74s 29ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0053 - val_accuracy: 0.9990\n",
      "Epoch 8/10\n",
      "2507/2507 [==============================] - 74s 30ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0045 - val_accuracy: 0.9990\n",
      "Epoch 9/10\n",
      "2507/2507 [==============================] - 75s 30ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0058 - val_accuracy: 0.9986\n",
      "Epoch 10/10\n",
      "2507/2507 [==============================] - 75s 30ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.0063 - val_accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa6605d26b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into train and test\n",
    "# X_train_network, X_test_network, y_train_network, y_test_network = train_test_split(\n",
    "#     transformed_x_network, y_network, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train_network = X_train_network.reshape(\n",
    "#     X_train_network.shape[0], X_train_network.shape[1], 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train,validation_split=0.2, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test_network.reshape( X_test_network.shape[0], X_test_network.shape[1], 1))\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test_network, preds.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming your data has the shape (9876, 7)\n",
    "# X_train is your input data, and y_train is your binary labels (0 or 1)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the input data to match the input shape of Conv1D layer\n",
    "X_train = transformed_x_network.reshape(\n",
    "    (transformed_x_network.shape[0], transformed_x_network.shape[1], 1))\n",
    "\n",
    "y_train = y_network\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 1D convolutional layer with 32 filters, kernel size 3, and ReLU activation\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu',\n",
    "          input_shape=(X_train.shape[1], 1)))\n",
    "# Add a max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Add another 1D convolutional layer with 64 filters, kernel size 3, and ReLU activation\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "# Add another max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Flatten the output for the fully connected layers\n",
    "model.add(Flatten())\n",
    "# Add a dense layer with 128 units and ReLU activation\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "# Add the output layer with one unit and a sigmoid activation function for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an optimizer of your choice\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
